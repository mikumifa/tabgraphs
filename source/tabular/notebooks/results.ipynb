{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9007d2ac-8a0e-4d95-b702-e5aaff5dba37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "_project_dir = os.path.dirname(os.getcwd())\n",
    "os.environ['PROJECT_DIR'] = _project_dir\n",
    "sys.path.append(_project_dir)\n",
    "del _project_dir\n",
    "\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from typing import Any, Optional, Union, cast\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import lib; lib.configure_libraries()\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f37562",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PROPERTIES = [\"task_type\", \"size\", \"n_features\"]\n",
    "_DATASETS_INFO: dict[Path, dict[str, Any]] = {}\n",
    "\n",
    "DATASETS_MAIN = [\n",
    "    # 'churn',\n",
    "    # 'california',\n",
    "    # 'house',\n",
    "    # 'adult',\n",
    "    # 'diamond',\n",
    "    # 'otto',\n",
    "    # 'higgs-small',\n",
    "    # 'black-friday',\n",
    "    # 'weather-small',\n",
    "    # 'covtype',\n",
    "    # 'microsoft',\n",
    "\n",
    "    ###\n",
    "\n",
    "    'tolokers-tab',\n",
    "    'questions-tab',\n",
    "    'city-reviews',\n",
    "    'browser-games',\n",
    "    'hm-categories',\n",
    "    'web-fraud',\n",
    "    'city-roads-M',\n",
    "    'city-roads-L',\n",
    "    'avazu-devices',\n",
    "    'hm-prices',\n",
    "    'web-traffic'\n",
    "]\n",
    "\n",
    "# The datasets from the paper \"Why do tree-based models still outperform deep learning on tabular data?\"\n",
    "DATASETS_WHY = [\n",
    "    'classif-cat-large-0-covertype',\n",
    "    'classif-cat-large-0-road-safety',\n",
    "    'classif-cat-medium-0-KDDCup09_upselling',\n",
    "    'classif-cat-medium-1-KDDCup09_upselling',\n",
    "    'classif-cat-medium-2-KDDCup09_upselling',\n",
    "    'classif-cat-medium-0-compass',\n",
    "    'classif-cat-medium-1-compass',\n",
    "    'classif-cat-medium-0-electricity',\n",
    "    'classif-cat-medium-0-rl',\n",
    "    'classif-cat-medium-1-rl',\n",
    "    'classif-cat-medium-2-rl',\n",
    "    'classif-num-large-0-Higgs',\n",
    "    'classif-num-large-0-MiniBooNE',\n",
    "    'classif-num-large-0-jannis',\n",
    "    'classif-num-medium-0-MagicTelescope',\n",
    "    'classif-num-medium-1-MagicTelescope',\n",
    "    'classif-num-medium-2-MagicTelescope',\n",
    "    'classif-num-medium-0-bank-marketing',\n",
    "    'classif-num-medium-1-bank-marketing',\n",
    "    'classif-num-medium-2-bank-marketing',\n",
    "    'regression-num-medium-0-california',\n",
    "    'classif-num-medium-0-credit',\n",
    "    'classif-num-medium-1-credit',\n",
    "    'regression-num-medium-0-house_16H',\n",
    "    'classif-num-medium-0-kdd_ipums_la_97-small',\n",
    "    'classif-num-medium-1-kdd_ipums_la_97-small',\n",
    "    'classif-num-medium-2-kdd_ipums_la_97-small',\n",
    "    'classif-num-medium-0-phoneme',\n",
    "    'classif-num-medium-1-phoneme',\n",
    "    'classif-num-medium-2-phoneme',\n",
    "    'classif-num-medium-3-phoneme',\n",
    "    'classif-num-medium-4-phoneme',\n",
    "    'regression-num-medium-0-pol',\n",
    "    'regression-num-medium-1-pol',\n",
    "    'classif-num-medium-0-wine',\n",
    "    'classif-num-medium-1-wine',\n",
    "    'classif-num-medium-2-wine',\n",
    "    'classif-num-medium-3-wine',\n",
    "    'classif-num-medium-4-wine',\n",
    "    'regression-cat-large-0-SGEMM_GPU_kernel_performance',\n",
    "    'regression-cat-large-0-black_friday',\n",
    "    'regression-cat-large-0-diamonds',\n",
    "    'regression-cat-large-0-nyc-taxi-green-dec-2016',\n",
    "    'regression-cat-large-0-particulate-matter-ukair-2017',\n",
    "    'regression-cat-medium-0-Bike_Sharing_Demand',\n",
    "    'regression-cat-medium-1-Bike_Sharing_Demand',\n",
    "    'regression-cat-medium-0-Brazilian_houses',\n",
    "    'regression-cat-medium-1-Brazilian_houses',\n",
    "    'regression-cat-medium-2-Brazilian_houses',\n",
    "    'regression-cat-medium-0-Mercedes_Benz_Greener_Manufacturing',\n",
    "    'regression-cat-medium-1-Mercedes_Benz_Greener_Manufacturing',\n",
    "    'regression-cat-medium-2-Mercedes_Benz_Greener_Manufacturing',\n",
    "    'regression-cat-medium-3-Mercedes_Benz_Greener_Manufacturing',\n",
    "    'regression-cat-medium-4-Mercedes_Benz_Greener_Manufacturing',\n",
    "    'regression-cat-medium-0-OnlineNewsPopularity',\n",
    "    'regression-cat-medium-0-analcatdata_supreme',\n",
    "    'regression-cat-medium-1-analcatdata_supreme',\n",
    "    'regression-cat-medium-2-analcatdata_supreme',\n",
    "    'regression-cat-medium-3-analcatdata_supreme',\n",
    "    'regression-cat-medium-4-analcatdata_supreme',\n",
    "    'regression-cat-medium-0-house_sales',\n",
    "    'regression-cat-medium-0-visualizing_soil',\n",
    "    'regression-cat-medium-1-visualizing_soil',\n",
    "    'regression-cat-medium-2-visualizing_soil',\n",
    "    'regression-cat-medium-0-yprop_4_1',\n",
    "    'regression-cat-medium-1-yprop_4_1',\n",
    "    'regression-cat-medium-2-yprop_4_1',\n",
    "    'regression-num-large-0-year',\n",
    "    'regression-num-medium-0-Ailerons',\n",
    "    'regression-num-medium-1-Ailerons',\n",
    "    'regression-num-medium-2-Ailerons',\n",
    "    'regression-num-medium-0-MiamiHousing2016',\n",
    "    'regression-num-medium-1-MiamiHousing2016',\n",
    "    'regression-num-medium-2-MiamiHousing2016',\n",
    "    'regression-num-medium-0-cpu_act',\n",
    "    'regression-num-medium-1-cpu_act',\n",
    "    'regression-num-medium-2-cpu_act',\n",
    "    'regression-num-medium-0-elevators',\n",
    "    'regression-num-medium-1-elevators',\n",
    "    'regression-num-medium-0-fifa',\n",
    "    'regression-num-medium-1-fifa',\n",
    "    'regression-num-medium-0-houses',\n",
    "    'regression-num-medium-0-isolet',\n",
    "    'regression-num-medium-1-isolet',\n",
    "    'regression-num-medium-2-isolet',\n",
    "    'regression-num-medium-0-medical_charges',\n",
    "    'regression-num-medium-0-sulfur',\n",
    "    'regression-num-medium-1-sulfur',\n",
    "    'regression-num-medium-2-sulfur',\n",
    "    'regression-num-medium-0-superconduct',\n",
    "    'regression-num-medium-0-wine_quality',\n",
    "    'regression-num-medium-1-wine_quality',\n",
    "    'regression-num-medium-2-wine_quality',\n",
    "]\n",
    "DATASETS_ALL = DATASETS_MAIN # + ['weather-big'] + DATASETS_WHY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2799e6f-7aba-42df-85a3-4961b9f02638",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset_info(dpath: Union[str, Path]) -> dict:\n",
    "    dpath = lib.get_path(dpath)\n",
    "    if dpath in _DATASETS_INFO:\n",
    "        return _DATASETS_INFO[dpath]\n",
    "\n",
    "    dataset = lib.Dataset.from_dir(dpath, None)\n",
    "    _DATASETS_INFO[dpath] = {\n",
    "        'dataset': (\n",
    "            # dpath.name.upper()[:2] if dpath.parent == lib.DATA_DIR and dpath.name in DATASETS_MAIN\n",
    "            dpath.name if dpath.parent == lib.DATA_DIR and dpath.name in DATASETS_MAIN\n",
    "            else 'WE (full)' if dpath.parent == lib.DATA_DIR and dpath.name == 'weather-big'\n",
    "            else dpath.name\n",
    "        ),\n",
    "        'task_type': dataset.task_type.value,\n",
    "        'size': dataset.size(None),\n",
    "        'n_features': dataset.n_features,   \n",
    "    }\n",
    "    return deepcopy(_DATASETS_INFO[dpath])\n",
    "\n",
    "\n",
    "def load_record(output: Union[str, Path]):\n",
    "    output = lib.get_path(output)\n",
    "    report = lib.load_report(output)\n",
    "    if lib.EXP_DIR in output.parents and '/exp/npt/' in str(output):\n",
    "        # The NPT reports do not follow the required format,\n",
    "        # so we infer the dataset path from the output path.\n",
    "        dpath = ':data/' + list(output.relative_to(lib.EXP_DIR / 'npt').parents)[-2].name\n",
    "    else:\n",
    "        if report[\"function\"] == 'bin.tune.main':\n",
    "            report = report[\"best\"]\n",
    "\n",
    "        if report[\"function\"] == 'bin.ensemble.main':\n",
    "            dpath = report[\"data\"]\n",
    "        else:\n",
    "            data = report[\"config\"][\"data\"]\n",
    "            dpath = data if isinstance(data, str) else data['path']\n",
    "            del data\n",
    "\n",
    "    record = get_dataset_info(dpath)\n",
    "    for part in lib.Part:\n",
    "        if part.value in report[\"metrics\"]:\n",
    "            # score = report[\"metrics\"][part.value][\"score\"]\n",
    "            # if record['dataset'] == 'HO':\n",
    "            #     # Prettify the score for \":data/house\".\n",
    "            #     score /= 10000\n",
    "            # record[f\"{part.value}_score\"] = score\n",
    "            metric = (\n",
    "                report[\"metrics\"][part.value][\"roc-auc\"]\n",
    "                if 'roc-auc' in report[\"metrics\"][part.value].keys()\n",
    "                \n",
    "                else report[\"metrics\"][part.value][\"r2\"]\n",
    "                if 'r2' in report[\"metrics\"][part.value].keys()\n",
    "                \n",
    "                else report[\"metrics\"][part.value][\"accuracy\"]\n",
    "            )\n",
    "            record[f\"{part.value}_score\"] = metric\n",
    "    return record\n",
    "\n",
    "\n",
    "def _compute_ranks(dataset_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    dataset_df = dataset_df.sort_values(['test_mean', 'test_std'], ascending=[False, True])\n",
    "    ranks = []\n",
    "    current_score = None\n",
    "    current_std = None\n",
    "    for _, columns in dataset_df.iterrows():\n",
    "        score = columns['test_mean']\n",
    "        std = columns['test_std']\n",
    "        if current_score is None:\n",
    "            ranks.append(1)\n",
    "            current_score = score\n",
    "            current_std = std\n",
    "        elif current_score - score <= current_std:\n",
    "            ranks.append(ranks[-1])\n",
    "        else:\n",
    "            ranks.append(ranks[-1] + 1)\n",
    "            current_score = score\n",
    "            current_std = std\n",
    "    dataset_df['rank'] = ranks\n",
    "    return dataset_df\n",
    "\n",
    "\n",
    "def build_metrics_dataframe(\n",
    "    outputs_info: list[\n",
    "        tuple[\n",
    "            Union[str, Path],  # output path\n",
    "            str,  # key (for example, algorithm name: \"MLP\")\n",
    "            Union[int, str],  # subkey for aggregation (for example, seed: 0)\n",
    "        ]\n",
    "    ],\n",
    "    precision: Optional[int] = 4,\n",
    "):\n",
    "    # >>> Build dataframe.\n",
    "    records = [\n",
    "        load_record(output) | { 'key': key, 'subkey': str(subkey)}\n",
    "        for output, key, subkey in outputs_info\n",
    "        if lib.get_path(output).joinpath('DONE').exists()\n",
    "    ]\n",
    "    if not records:\n",
    "        raise RuntimeError('No records are available')\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    has_train_score = 'train_score' in df.columns\n",
    "\n",
    "    # >>> Aggregate over subkeys.\n",
    "    aggregations = {\n",
    "        'test_mean': (\"test_score\", \"mean\"),\n",
    "        'test_std': (\"test_score\", \"std\"),\n",
    "        'val_mean': (\"val_score\", \"mean\"),\n",
    "        'val_std': (\"val_score\", \"std\"),\n",
    "    }\n",
    "    if has_train_score:\n",
    "        aggregations.update({\n",
    "            'train_mean': (\"train_score\", \"mean\"),\n",
    "            'train_std': (\"train_score\", \"std\"),\n",
    "        })\n",
    "    aggregations['count'] = (\"test_score\", \"count\")\n",
    "    aggregations.update({\n",
    "        x: (x, \"first\")\n",
    "        for x in DATASET_PROPERTIES\n",
    "        if x in df.columns\n",
    "    })\n",
    "    df = df.groupby([\"dataset\", \"key\"]).agg(**aggregations)\n",
    "    df = df.reset_index().fillna(0.0)\n",
    "    df[\"count\"] = df[\"count\"].astype(int)\n",
    "\n",
    "    # >>> Compute ranks.\n",
    "    df = cast(\n",
    "        pd.DataFrame,\n",
    "        df.groupby(['dataset'], group_keys=False).apply(_compute_ranks)\n",
    "    )\n",
    "\n",
    "    # >>> Finalize.\n",
    "    df = df.sort_values(\n",
    "        ['size', 'n_features', 'dataset', 'test_mean'],\n",
    "        ascending=[True, True, True, False],\n",
    "    ).reset_index(drop=True)\n",
    "    # df.loc[\n",
    "    #     df['task_type'] == 'regression',\n",
    "    #     ['test_mean', 'val_mean'] + ['train_mean'] * int(has_train_score)\n",
    "    # ] *= -1\n",
    "    if precision is not None:\n",
    "        float_columns = [\n",
    "            'test_mean', 'test_std',\n",
    "            'val_mean', 'val_std',\n",
    "        ] + ['train_mean', 'train_std'] * int(has_train_score)\n",
    "        df[float_columns] = df[float_columns].round(precision)\n",
    "    df = df.set_index([\"dataset\"] + DATASET_PROPERTIES + [\"key\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def summarize_ranks(metrics_df: pd.DataFrame, nans: bool) -> pd.DataFrame:\n",
    "    df = metrics_df\n",
    "    df = df.reset_index().pivot(index='key', columns='dataset', values='rank')\n",
    "    if not nans:\n",
    "        df = df.dropna(axis='columns')\n",
    "    columns = df.columns.tolist()\n",
    "    df[\"avg\"] = df.mean(1)\n",
    "    df[\"std\"] = df.std(1)\n",
    "    df.insert(0, \"avg\", df.pop(\"avg\").round(1))\n",
    "    df.insert(1, \"std\", df.pop(\"std\").round(1))\n",
    "    df = df.sort_values(\"avg\")\n",
    "    df = df[['avg', 'std'] + columns]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295fd0b3-0b13-4b7a-8bed-2cec81dad060",
   "metadata": {
    "tags": []
   },
   "source": [
    "# How to use the next cell\n",
    "- comment/uncomment `N_SEEDS += 15` to show/hide results for single models\n",
    "- comment/uncomment `N_ENSEMBLES += 3` to show/hide results for ensembles\n",
    "- in the `for dataset in datasets` loop:\n",
    "    - comment/uncomment the `add(...)` lines to show/hide results for various algorithms\n",
    "    - in particular, uncomment `add(f':exp/mlp/{dataset}/0-reproduce', 'MLP (reproduce)')` to complete the tutorial from `README.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b900ea-8a57-4e9b-ac2d-3021f3d19882",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_SEEDS = 0\n",
    "N_SEEDS += 5\n",
    "N_ENSEMBLES = 0\n",
    "# N_ENSEMBLES += 3\n",
    "\n",
    "# See the comments in build_metrics_dataframe to learn about outputs_info.\n",
    "outputs_info = []\n",
    "def add(location: str, name: Optional[str] = None, sep: str = '-'):\n",
    "    if name is None:\n",
    "        assert location.startswith(':exp/')\n",
    "        # location example: \":exp/mlp/california/0\"\n",
    "        _exp_prefix, alg, *_dataset, tag = location.split('/')\n",
    "        name = f'{alg}[{tag}]'\n",
    "    for seed in range(N_SEEDS):\n",
    "        outputs_info.append((location + f'{sep}evaluation/{seed}', name, seed))\n",
    "    for ensemble_i in range(N_ENSEMBLES):\n",
    "        outputs_info.append((location + f'{sep}ensemble{sep}5/{ensemble_i}', '(E) ' + name, ensemble_i))\n",
    "\n",
    "datasets = DATASETS_MAIN\n",
    "for dataset in datasets:\n",
    "    if dataset in DATASETS_WHY:\n",
    "        dataset = 'why/' + dataset\n",
    "\n",
    "    # >>> Tutorial from README.md\n",
    "    # add(f':exp/mlp/{dataset}/0-reproduce', 'MLP (reproduce)')\n",
    "\n",
    "    # >>> Retrieval-augmented baselines\n",
    "    # add(f':exp/knn/{dataset}/0', 'kNN')\n",
    "\n",
    "    # dnnr_tag = 'ohe' if dataset in [BLACK_FRIDAY, DIAMOND] else 'loo'\n",
    "    # add(f':exp/dnnr/{dataset}/{dnnr_tag}', 'DNNR')\n",
    "\n",
    "    # add(f':exp/anp/{dataset}/0', 'ANP')\n",
    "    # add(f':exp/dkl/{dataset}/0', 'DKL')\n",
    "\n",
    "    # npt_tag = {\n",
    "    #     'churn': 0,\n",
    "    #     'california': 0,\n",
    "    #     'house': 0,\n",
    "    #     'adult': 0,\n",
    "    #     'diamond': 2,\n",
    "    #     'otto': 1,\n",
    "    #     'higgs-small': 2,\n",
    "    #     'black-friday': 2,\n",
    "    #     'covtype': 3,\n",
    "    #     'weather-small': 1,\n",
    "    #     'microsoft': 1,\n",
    "    # }[dataset]\n",
    "    # add(f':exp/npt/{dataset}/{npt_tag}', 'NPT')\n",
    "\n",
    "    # saint_tag = 'default' if dataset in ['weather-small', 'covtype', 'microsoft'] else '2'\n",
    "    # add(f':exp/saint/{dataset}/{saint_tag}', 'SAINT')\n",
    "\n",
    "    # >>> Parametric DL baselines\n",
    "    # add(f':exp/mlp/{dataset}/0', 'MLP')\n",
    "    # add(f':exp/mlp/{dataset}/lr', 'MLP-LR')\n",
    "    # add(f':exp/mlp/{dataset}/plr-lite', 'MLP-PLR(lite)')\n",
    "    # add(f':exp/mlp/{dataset}/plr', 'MLP-PLR')\n",
    "    add(f':exp/mlp/{dataset}/plr-lite', 'MLP-PLR')\n",
    "\n",
    "    # >>> GBDT\n",
    "    # add(f':exp/xgboost_/{dataset}/default2', 'XGBoost (default)')\n",
    "    # add(f':exp/lightgbm_/{dataset}/default2', 'LightGBM (default)')\n",
    "    # add(f':exp/catboost_/{dataset}/default2', 'CatBoost (default)')\n",
    "    # add(f':exp/xgboost_/{dataset}/2', 'XGBoost')\n",
    "    # add(f':exp/lightgbm_/{dataset}/2', 'LightGBM')\n",
    "    # add(f':exp/catboost_/{dataset}/2', 'CatBoost')\n",
    "    add(f':exp/catboost_/{dataset}/test', 'CatBoost')\n",
    "    add(f':exp/xgboost_/{dataset}/test', 'XGBoost')\n",
    "    add(f':exp/lightgbm_/{dataset}/test', 'LightGBM')\n",
    "\n",
    "    # >>> The model\n",
    "    model = 'TabR'\n",
    "    modeldir = model.lower()\n",
    "    # add(f':exp/{modeldir}/{dataset}/default', f'{model}-S (default)')\n",
    "    # add(f':exp/{modeldir}/{dataset}/0', f'{model}-S')\n",
    "    # model_tag = \"2-lr\" if dataset in ['weather-small', 'covtype', 'microsoft'] else \"2-plr-lite\"\n",
    "    # add(f':exp/{modeldir}/{dataset}/{model_tag}', f'{model}')\n",
    "    add(f':exp/{modeldir}/{dataset}/plr-lite', f'{model}')\n",
    "\n",
    "    # >>> Ablation study\n",
    "    for tag, name in [\n",
    "        # ('dp-qk-v-self-scaled', 'Step-0'),\n",
    "        # ('dp-qk-yv-self-scaled', 'Step-1'),\n",
    "        # ('l2-k-yv-self-scaled', 'Step-2'),\n",
    "        # ('l2-k-yt-self-scaled', 'Step-3'),\n",
    "    ]:\n",
    "        add(f':exp/{modeldir}_design/{dataset}/{tag}', f'(design) {name}')\n",
    "\n",
    "    # >>> Context freeze\n",
    "    for freeze_after_n_epochs in [\n",
    "        # 0,\n",
    "        # 1,\n",
    "        # 2,\n",
    "        # 4,\n",
    "        # 5,\n",
    "        # 8,\n",
    "    ]:\n",
    "        add(f':exp/{modeldir}_scaling/{dataset}/default-freeze-{freeze_after_n_epochs}', f'{model}-freeze-{freeze_after_n_epochs}')\n",
    "\n",
    "metrics_df = build_metrics_dataframe(outputs_info)\n",
    "# Drop details about datasets to save screen space.\n",
    "# while len(metrics_df.index.levels) > 2:\n",
    "#     metrics_df.index = metrics_df.index.droplevel(1)\n",
    "ranks_df = summarize_ranks(metrics_df, nans=True)\n",
    "print('Ranks:')\n",
    "display(ranks_df)\n",
    "print('\\nMetrics:')\n",
    "display(metrics_df)\n",
    "# metrics_df.to_html('metrics.html')\n",
    "# ranks_df.to_html('metrics.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a5c83d",
   "metadata": {},
   "source": [
    "# [CUSTOM] Results for Tabular DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf3e294",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_column_names = [\n",
    "    'test_mean',\n",
    "    'test_std',\n",
    "]\n",
    "df_metrics = metrics_df[required_column_names].copy()\n",
    "df_metrics *= 100.\n",
    "df_metrics = df_metrics.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1e3f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.index = df_metrics.index.droplevel(level=3).droplevel(level=2).droplevel(level=1)\n",
    "df_metrics.index.names = ['dataset', 'model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfe6d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce73e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_metrics = df_metrics['test_mean'].apply(lambda entry: f'{entry:.2f}') + ' \\pm ' + df_metrics['test_std'].apply(lambda entry: f'{entry:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ebacc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_processed = sr_metrics.unstack(level=0)\n",
    "df_metrics_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26df38f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_processed.index.name = None\n",
    "df_metrics_processed.columns.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdde87ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_dataset_order = [\n",
    "    'tolokers-tab',\n",
    "    'questions-tab',\n",
    "    'city-reviews',\n",
    "    'browser-games',\n",
    "    'hm-categories',\n",
    "    'city-roads-M',\n",
    "    'city-roads-L',\n",
    "    'avazu-devices',\n",
    "    'hm-prices',\n",
    "]\n",
    "\n",
    "df_metrics_processed = df_metrics_processed[correct_dataset_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e63f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dd7cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_style = df_metrics_processed.style\n",
    "df_metrics_style.format(lambda entry: f'${entry}$' if not pd.isnull(entry) else 'MLE')\n",
    "\n",
    "print(df_metrics_style.to_latex(\n",
    "    column_format='l' + 'c' * len(df_metrics_style.columns)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39053e5",
   "metadata": {},
   "source": [
    "# [CUSTOM] Results for BGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3566e6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_root = '../../bgnn/results'\n",
    "result_dict = {}\n",
    "\n",
    "for dataset_name in DATASETS_ALL:\n",
    "    result_path = f'{result_root}/{dataset_name}/aggregated_results.json'\n",
    "    with open(result_path, 'r') as f:\n",
    "        report = np.array(list(json.load(f).values())).squeeze()\n",
    "        result_dict[dataset_name] = {\n",
    "            'test_mean': report[0] * 100., 'test_std': report[1] * 100.\n",
    "        }\n",
    "\n",
    "df_metrics = pd.DataFrame(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a94786",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4684c274",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_metrics = (\n",
    "    df_metrics.loc['test_mean'].apply(lambda entry: f'{entry:.2f}')\n",
    "    + ' \\pm ' + \n",
    "    df_metrics.loc['test_std'].apply(lambda entry: f'{entry:.2f}')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f617e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45412c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_processed = pd.DataFrame({'BGNN': sr_metrics}).T\n",
    "df_metrics_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6ea1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_dataset_order = [\n",
    "    'tolokers-tab',\n",
    "    'questions-tab',\n",
    "    'city-reviews',\n",
    "    'browser-games',\n",
    "    'hm-categories',\n",
    "    'city-roads-M',\n",
    "    'city-roads-L',\n",
    "    'avazu-devices',\n",
    "    'hm-prices',\n",
    "]\n",
    "\n",
    "df_metrics_processed = df_metrics_processed[correct_dataset_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2844379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_style = df_metrics_processed.style\n",
    "df_metrics_style.format(lambda entry: f'${entry}$' if not pd.isnull(entry) else 'OOM')\n",
    "\n",
    "print(df_metrics_style.to_latex(\n",
    "    column_format='l' + 'c' * len(df_metrics_style.columns)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea52fc8f",
   "metadata": {},
   "source": [
    "# [CUSTOM] Results for EBBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91460e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_root = '../../ebbs/results'\n",
    "result_dict = {}\n",
    "\n",
    "for dataset_name in DATASETS_ALL:\n",
    "    result_path = f'{result_root}/{dataset_name}/aggregated.json'\n",
    "    with open(result_path, 'r') as f:\n",
    "        report = np.array(list(json.load(f).values())).squeeze()\n",
    "        result_dict[dataset_name] = {\n",
    "            'test_mean': report[0] * 100., 'test_std': report[1] * 100.\n",
    "        }\n",
    "\n",
    "df_metrics = pd.DataFrame(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5e5046",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_metrics = (\n",
    "    df_metrics.loc['test_mean'].apply(lambda entry: f'{entry:.2f}')\n",
    "    + ' \\pm ' + \n",
    "    df_metrics.loc['test_std'].apply(lambda entry: f'{entry:.2f}')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916b72cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a979b9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_processed = pd.DataFrame({'EBBS': sr_metrics}).T\n",
    "df_metrics_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bb37b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_dataset_order = [\n",
    "    'tolokers-tab',\n",
    "    'questions-tab',\n",
    "    'city-reviews',\n",
    "    'browser-games',\n",
    "    'hm-categories',\n",
    "    'city-roads-M',\n",
    "    'city-roads-L',\n",
    "    'avazu-devices',\n",
    "    'hm-prices',\n",
    "]\n",
    "\n",
    "df_metrics_processed = df_metrics_processed[correct_dataset_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae227cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_style = df_metrics_processed.style\n",
    "df_metrics_style.format(lambda entry: f'${entry}$' if not pd.isnull(entry) else 'OOM')\n",
    "\n",
    "print(df_metrics_style.to_latex(\n",
    "    column_format='l' + 'c' * len(df_metrics_style.columns)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b34975d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad1c4ccb4b0f87320904f04a0667e53dc3875ea3cc5dc70aa6dac74e1b6b3256"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
