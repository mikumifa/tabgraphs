{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [\n",
    "    # 'tolokers-tab',\n",
    "    # 'questions-tab',\n",
    "    # 'city-reviews',\n",
    "    # 'browser-games',\n",
    "    # 'hm-categories',\n",
    "    # 'web-fraud',\n",
    "    # 'city-roads-M',\n",
    "    # 'city-roads-L',\n",
    "    # 'avazu-devices',\n",
    "    # 'hm-prices',\n",
    "    # 'web-traffic',\n",
    "]\n",
    "\n",
    "data_root = '../datasets'\n",
    "tabular_root = '../source/tabular/data'\n",
    "specialized_root = '../source/bgnn/datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path):\n",
    "    with open(config_path, 'r') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Datasets for Tabular Baselines (GBDT, MLP, TabR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_DWE = False\n",
    "USE_NFA = False\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print(dataset_name)\n",
    "    \n",
    "    tabular_dataset_path = f'{tabular_root}/{dataset_name}'\n",
    "    if USE_DWE: tabular_dataset_path += 'dwe'\n",
    "    if USE_NFA: tabular_dataset_path += 'nfa'\n",
    "\n",
    "    os.makedirs(tabular_dataset_path, exist_ok=True)\n",
    "\n",
    "    dataset_path = f'{data_root}/{dataset_name}'\n",
    "\n",
    "    ### convert data\n",
    "\n",
    "    df_features = pd.read_csv(f'{dataset_path}/features.csv', index_col=0)\n",
    "    df_targets = pd.read_csv(f'{dataset_path}/targets.csv', index_col=0)\n",
    "\n",
    "    dataset_info = load_config(f'{dataset_path}/info.yaml')\n",
    "    masks = {}\n",
    "    for part_name in ['train', 'valid', 'test']:\n",
    "        masks[part_name] = pd.read_csv(f'{dataset_path}/{part_name}_mask.csv', index_col=0).values.reshape(-1)\n",
    "\n",
    "    num_features = (\n",
    "        df_features[dataset_info['num_feature_names']].values.astype(np.float32)\n",
    "        if dataset_info['num_feature_names'] else None\n",
    "    )\n",
    "\n",
    "    cat_features = (\n",
    "        df_features[dataset_info['cat_feature_names']].values.astype(np.int32).astype(np.str_)\n",
    "        if dataset_info['cat_feature_names'] else None\n",
    "    )\n",
    "\n",
    "    bin_features = (\n",
    "        df_features[dataset_info['bin_feature_names']].values.astype(np.float32)\n",
    "        if dataset_info['bin_feature_names'] else None\n",
    "    )\n",
    "\n",
    "    targets = df_targets[dataset_info['target_name']].values\n",
    "    info = {\n",
    "        'task_type': 'regression' if dataset_info['task'] == 'regression' \n",
    "            else 'binclass' if dataset_info['task'] == 'binary_classification' \n",
    "            else 'multiclass',\n",
    "        'name': dataset_name,\n",
    "    }\n",
    "\n",
    "    ### append graph augmented features and node embeddings\n",
    "\n",
    "    if USE_DWE:\n",
    "        df_augmented_features = pd.read_csv(f'{dataset_path}/augmented_features.csv', index_col=0)\n",
    "        extra_num_features = df_augmented_features.values.astype(np.float32)\n",
    "        num_features = (\n",
    "            np.concatenate([num_features, extra_num_features], axis=1) \n",
    "            if num_features is not None else extra_num_features\n",
    "        )\n",
    "    \n",
    "    if USE_NFA:\n",
    "        node_embeds = np.load(f'{dataset_path}/node_embeddings.npz')['node_embeds'].astype(np.float32)\n",
    "        num_features = (\n",
    "            np.concatenate([num_features, node_embeds], axis=1) \n",
    "            if num_features is not None else node_embeds\n",
    "        )\n",
    "    \n",
    "    ### write data to files\n",
    "    \n",
    "    with open(f'{tabular_dataset_path}/info.json', 'w') as f:\n",
    "        json.dump(info, f)\n",
    "\n",
    "    for part_name, conventional_part_name in zip(\n",
    "        ['train', 'valid', 'test'], ['train', 'val', 'test']\n",
    "    ):\n",
    "        mask = masks[part_name]\n",
    "        if num_features is not None:\n",
    "            np.save(f'{tabular_dataset_path}/X_num_{conventional_part_name}.npy', num_features[mask])\n",
    "        \n",
    "        if cat_features is not None:\n",
    "            np.save(f'{tabular_dataset_path}/X_cat_{conventional_part_name}.npy', cat_features[mask])\n",
    "        \n",
    "        if bin_features is not None:\n",
    "            np.save(f'{tabular_dataset_path}/X_bin_{conventional_part_name}.npy', bin_features[mask])\n",
    "\n",
    "        targets_masked = targets[mask]\n",
    "        targets_casted = (\n",
    "            targets_masked.astype(np.int32) if dataset_info['task'] != 'regression' \n",
    "            else targets_masked.astype(np.float32)\n",
    "        )\n",
    "        \n",
    "        np.save(f'{tabular_dataset_path}/Y_{conventional_part_name}.npy', targets_casted)\n",
    "    \n",
    "    open(f'{tabular_dataset_path}/READY', 'a').close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Datasets for Specialized Models (BGNN and EBBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_DWE = False\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    print(dataset_name)\n",
    "    \n",
    "    specialized_dataset_path = f'{specialized_root}/{dataset_name}'\n",
    "    os.makedirs(specialized_dataset_path, exist_ok=True)\n",
    "\n",
    "    dataset_path = f'{data_root}/{dataset_name}'\n",
    "\n",
    "    ### convert data\n",
    "\n",
    "    df_features = pd.read_csv(f'{dataset_path}/features.csv', index_col=0)\n",
    "    dataset_info = load_config(f'{dataset_path}/info.yaml')\n",
    "    edgelist = pd.read_csv(f'{dataset_path}/edgelist.csv')\n",
    "    \n",
    "    split_indices = {}\n",
    "    for part_name in ['train', 'valid', 'test']:\n",
    "        mask = pd.read_csv(f'{dataset_path}/{part_name}_mask.csv', index_col=0).values.reshape(-1)\n",
    "        split_indices[part_name] = np.where(mask)[0]\n",
    "\n",
    "    targets = df_features[dataset_info['target_name']]\n",
    "    features = df_features[\n",
    "        dataset_info['num_feature_names'] + \n",
    "        dataset_info['cat_feature_names'] +\n",
    "        dataset_info['bin_feature_names']\n",
    "    ]\n",
    "\n",
    "    if USE_DWE:\n",
    "        node_embeds = np.load(f'{dataset_path}/node_embeddings.npz')['node_embeds'].astype(np.float32)\n",
    "        df_node_embeds = pd.DataFrame(\n",
    "            node_embeds, columns=[\n",
    "                f'embed_coordinate_{idx}' \n",
    "                for idx in range(node_embeds.shape[1])\n",
    "            ]\n",
    "        )\n",
    "        features = pd.concat([features, df_node_embeds], axis=1)\n",
    "    \n",
    "    ### write data to files\n",
    "\n",
    "    with open(f'{specialized_dataset_path}/cat_features.txt', 'w') as f:\n",
    "        f.write('\\n'.join(dataset_info['cat_feature_names']) + '\\n')\n",
    "\n",
    "    with open(f'{specialized_dataset_path}/masks.json', 'w') as f:\n",
    "        json.dump({\n",
    "            conventional_part_name: split_indices[part_name].tolist()\n",
    "            for part_name, conventional_part_name in zip(\n",
    "                ['train', 'valid', 'test'], ['train', 'val', 'test']\n",
    "            )\n",
    "        }, f)\n",
    "\n",
    "    features.to_csv(f'{specialized_dataset_path}/X.csv', index=None)\n",
    "    targets.to_csv(f'{specialized_dataset_path}/y.csv', index=None)\n",
    "    edgelist.to_csv(f'{specialized_dataset_path}/edgelist.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabular",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
